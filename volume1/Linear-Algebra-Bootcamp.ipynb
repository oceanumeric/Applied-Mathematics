{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Bootcamp\n",
    "\n",
    "This post provides a quick overview of linear algebra, focusing on the\n",
    "mathematical ideas and definitions that are foundational for the rest \n",
    "of the series of posts. The post follows the book by Darve, E., & Wootters, M. (2021)\n",
    ": Numerical Linear Algebra with Julia. However, it will use [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) to implement essential algorithms. \n",
    "\n",
    "The book by Darve and Wootters is very good but expensive, I recommend [buying\n",
    "it](https://amzn.eu/d/8o3qbUI) if you are rich. Otherwise, just read my post. \n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/darve-wootters.webp\" alt=\"darve-wootters\" width=\"30%\">\n",
    "\n",
    "Many key concepts are omitted from this chapter as it assumes \n",
    "readers should have some basic knowledge about linear algebra. For instance,\n",
    "you should know the definition of _linearly independent_, _basis_ and\n",
    "_dot product_, etc. \n",
    "\n",
    "The angle $\\theta$ between two vectors $x, y \\in $\\mathbb{R}^n$ satisfies\n",
    "\n",
    "$$x^T y = ||x||_2 ||y||_2 \\cos \\theta $$\n",
    "\n",
    "or in a less rigorous way of expressing\n",
    "\n",
    "$$\\cos \\theta = \\frac{x^T y }{||x|| ||y||}$$\n",
    "\n",
    "A set of vectors $x_1, \\cdots, x_k$ are __orthogonal vectors__ if they are\n",
    "pairwise orthogonal, that is, if $x_i^T x_j = 0$ for all $i \\neq j$. For \n",
    "instance, if $x$ and $y$ are orthogonal, then \n",
    "\n",
    "$$||x+y||_2^2 = ||x||^2 + ||y||^2$$\n",
    "\n",
    "For the subspace $S$ of $\\mathbb{R}^n$, we define the __orthogonal complement__\n",
    "of $S$, denoted $S^{\\perp}$, to be the set of vectors in $\\mathbb{R}^n$ which\n",
    "are orthogonal to $S$.\n",
    "\n",
    "This means we have \n",
    "\n",
    "$$\\text{dim}(S) + \\text{dim}(S^{\\perp}) = n $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the __Cauchy-Schwarz__ inequality (and the more general __Hölder's inequality__)\n",
    "to bound a dot product of vectors by the norms of the vectors. \n",
    "\n",
    "Hölder's inequality says that for any vectors $x, y$ and positive numbers\n",
    "$p, q$ so that $1/p + 1/q = 1$, \n",
    "\n",
    "$$|x^Ty| \\leq ||x||_p ||y||_q$$\n",
    "\n",
    "When specialized to $p = q = 2$. We get the __Cauchy-Schwarz inequality__. For\n",
    "any vectors $x$ and $y$, \n",
    "\n",
    "$$|x^Ty| \\leq ||x||_2 ||y||_2 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complex matrix, we define the __conjugate transpose__ of $A$, denoted \n",
    "$A^H$, to be the matrix that we get when we take the entry-wise complex conjugate\n",
    "of $A^T$. That is, the $(i, j)$ entry of $A^H$ is $\\overline{a_{ji}}$\n",
    "\n",
    "Suppose we want to calculate the conjugate transpose of the \n",
    "following matrix $\\boldsymbol{A}$.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\\left[\\begin{array}{ccc}\n",
    "1 & -2-i & 5 \\\\\n",
    "1+i & i & 4-2 i\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "We first transpose the matrix:\n",
    "$$\n",
    "\\boldsymbol{A}^{\\top}=\\left[\\begin{array}{cc}\n",
    "1 & 1+i \\\\\n",
    "-2-i & i \\\\\n",
    "5 & 4-2 i\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Then we conjugate every entry of the matrix:\n",
    "$$\n",
    "\\boldsymbol{A}^{\\mathrm{H}}=\\left[\\begin{array}{cc}\n",
    "1 & 1-i \\\\\n",
    "-2+i & -i \\\\\n",
    "5 & 4+2 i\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __symmetric__ matrix is such t hat \n",
    "\n",
    "$$A^T = A$$\n",
    "\n",
    "A __skew-symmetric matrix__ is such that \n",
    "\n",
    "$$A^T = - A$$\n",
    "\n",
    "A __Hermitian__ matrix is such that \n",
    "\n",
    "$$A^H = A$$\n",
    "\n",
    "A __skew-Hermitian__ matrix has \n",
    "\n",
    "$$A^H = -A$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra of matrix\n",
    "\n",
    "$$(AB)C = A(BC)$$\n",
    "\n",
    "$$(AB)^T = B^T A^T$$\n",
    "\n",
    "$$(A \\pm B)^T = A^T \\pm B^T$$\n",
    "\n",
    "$$A^{-1} A = AA^{-1} = I $$\n",
    "\n",
    "If $A$ is square and not invertible, we say that it is __singular__. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sherman-Morrison-Woodbury formula\n",
    "\n",
    "We will learn different ways to calculate matrix inverse later, but here is\n",
    "one useful tool that you can think about now. \n",
    "\n",
    "If $A \\in \\mathbb{R}^{n \\times n}$, and $U, V \\in \\mathbb{R}^{n \\times k}$:\n",
    "\n",
    "$$(A + UV^T)^{-1} = A^{-1} - A^{-1} U(I + V^TA^{-1}U)^{-1} V^T A^{-1}$$\n",
    "\n",
    "where we assume that $A$ and $I + V^T A^{-1} U$ are non-singular."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix norms \n",
    "\n",
    "The Frobenius norm of $A$ is defined by\n",
    "$$\n",
    "\\|A\\|_F=\\left(\\sum_{i j} a_{i j}^2\\right)^{1 / 2}=\\sqrt{\\operatorname{tr}\\left(A A^H\\right)},\n",
    "$$\n",
    "where $A^H$ is the transpose conjugate matrix.\n",
    "\n",
    "The trace of a square matrix $A$ is defined as $\\text{tr}(A) = \\sum_i a_{ii}$\n",
    "\n",
    "For any invertible matrix $P$,\n",
    "\n",
    "$$\\text{tr}(A) = \\text{tr}(P^{-1}AP)$$\n",
    "\n",
    "We also have \n",
    "\n",
    "$$\\text{tr}(AB) = \\text{tr}(BA)$$\n",
    "\n",
    "$$\\text{tr}(A(BC)) = \\text{tr}((BC)A)$$\n",
    "\n",
    "$$\\text{tr}(A + \\alpha B) = \\text{tr}(A) + \\alpha \\ \\text{tr}(B)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The determinant of a matrix \n",
    "\n",
    "The determinant of matrix $A$ is equal to\n",
    "$$\n",
    "\\operatorname{det}(A)=\\sum_{\\sigma \\in S_n} \\operatorname{sign}(\\sigma) \\prod_{i=1}^n a_{i, \\sigma_i},\n",
    "$$\n",
    "where $S_n$ is the set of all permutations of $\\{1, \\ldots, n\\}$, and $\\operatorname{sign}(\\sigma)$ is the signature of the permutation $\\sigma$. Here, the signature of a permutation $\\sigma$ is $+1$ if $\\sigma$ can be realized by doing an even number of pairwise swaps, and it is $-1$ otherwise; it turns out (though it's not obvious!) that this is well-defined.\n",
    "\n",
    "The determinant satisfies the following three fundamental properties:\n",
    "1. $\\operatorname{det}\\left(I_n\\right)=1$.\n",
    "\n",
    "2. The determinant is an $n$-linear function. That is, if we \"fix\" all the columns \n",
    "of $A$ except column $i, \\operatorname{det}(A)$ is a linear function of $a_i$.\n",
    "\n",
    "3. The determinant is an alternating form. That is, when two columns of $A$ are identical, then $\\operatorname{det}(A)=0$.\n",
    "\n",
    "In fact, these three properties uniquely define the determinant \n",
    "(that is, the determinant is the only function satisfying these three properties).\n",
    "\n",
    "1. For any square matrix $A$ and scalar $\\alpha \\in \\mathbb{R}$, $\\operatorname{det}(\\alpha A)=\\alpha^n \\operatorname{det}(A)$.\n",
    "2. For any square matrices $A$ and $B, \\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B)$.\n",
    "3. A square matrix $A$ is singular (that is, not invertible) if and only if $\\operatorname{det}(A)=0$.\n",
    "4. $\\operatorname{det}(A)=\\operatorname{det}\\left(A^T\\right)$.\n",
    "5. $\\operatorname{det}\\left(A^{-1}\\right)=\\operatorname{det}(A)^{-1}$.\n",
    "\n",
    "For any square matrix $A$:\n",
    "\n",
    "$$\\operatorname{det}(\\text{exp}A) = \\text{exp}(\\text{tr} A)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal matrices and projectors \n",
    "\n",
    "An orthogonal matrix $Q \\in \\mathbb{R}^{m \\times n}$ is a matrix whose columns are orthonormal. That is, the columns $q_1, \\ldots, q_n$ of $Q$ satisfy\n",
    "$$\n",
    "q_i^T q_j=\\delta_{i j} .\n",
    "$$\n",
    "Equivalently,\n",
    "$$\n",
    "Q^T Q=I\n",
    "$$\n",
    "\n",
    "A __unitary matrxi__ $U \\in \\mathbb{C}^{n \\times n}$ is a matrix so that\n",
    "\n",
    "$$U^HU = UU^H = I $$\n",
    "\n",
    "Multiplication by an orthogonal matrix $Q$ preserves 2-norms:\n",
    "\n",
    "$$||Qx||_2^2 = x^T Q^T Q x = x^Tx = ||x||_2^2 $$\n",
    "\n",
    "__Square orthogonal__ matrices are always invertible by definition, and \n",
    "\n",
    "$$Q^{-1} = Q^{T}$$\n",
    "\n",
    "As a result, for square matrices, we have \n",
    "\n",
    "$$\\text{det}(Q) = \\pm 1$$\n",
    "\n",
    "\n",
    "Note that if $Q$ is square orthogonal, then \n",
    "\n",
    "$$Q^TQ = QQ^T = I$$\n",
    "\n",
    "If $Q$ is rectangular orthogonal, we only have \n",
    "\n",
    "$$Q^TQ = I$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the vector of response values is denoted by $y$ and the vector of \n",
    "fitted values by $\\hat{y}$\n",
    "\n",
    "$$\\hat{y} = Py$$\n",
    "\n",
    "The vector of residuals $r$ can also be expressed compactly using the \n",
    "__projection matrix__:\n",
    "\n",
    "$$r = y-\\hat{y} = y - Py = (I-P)y$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues, eigenvectors, and related matrix decompositions \n",
    "\n",
    "In this section, we will introduce __eigenvalues__ and __eigenvectors__,\n",
    "fundamental tools in linear algebra. Using these, we will define a number\n",
    "of different __matrix decompositions__, that is, ways to decompose a \n",
    "matrix as a product of easy-to-understand matrices. Later you will realize\n",
    "that matrix decompositions are an essential part of numerical linear\n",
    "algebra. \n",
    "\n",
    "Easy-to-understand matrices include:\n",
    "\n",
    "- __Diagonal matrices__, which have non-zero entries only on the diagonal. \n",
    "These are very easy to work with!\n",
    "\n",
    "- __Orthogonal matrices__, which are easier to invert (take the transpose!).\n",
    "As we will see, orthogonal matrices are also less prone to inaccuracies in \n",
    "computation. \n",
    "\n",
    "- __Triangular matrices__, which can be either __lower-triangular__ \n",
    "(with non-zero entries on or below the diagonal only) or __upper-triangular__\n",
    "(non-zero entries on or above the diagonal). As we will see, it can be more \n",
    "computationally efficient to deal with triangular matrices than with \n",
    "general matrices. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{C}^{n \\times n}$ be a __square__ matrix, and let \n",
    "$x \\in \\mathbb{C}^n$ be non-zero and $\\lambda \\in \\mathbb{C}$.\n",
    "We say that $x$ is an __eigenvector__ of $A$ with __eigenvalue__ $\\lambda$\n",
    "if \n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "\n",
    "The determinant and trace can be expressed in terms of the eigenvalues:\n",
    "\n",
    "$$\\text{det}(A) = \\prod_{i=1}^n \\lambda_i, \\ \\text{tr}(A) = \\sum_{i=1}^n \\lambda_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrices $A$ have $n$ linearly independent eigenvectors $x_1, \\ldots, x_n$. If this is the case, we say that $A$ is diagonalizable. The reason for the name is that if $A$ has $n$ linearly independent eigenvectors, we can decompose it as follows. Say that $\\lambda_1, \\ldots, \\lambda_n$ are the corresponding eigenvalues, and let $\\Lambda$ be the diagonal matrix whose $i$ th diagonal entry is $\\lambda_i$, and let $X$ be the matrix with the $x_i$ as columns. In matrix notation,\n",
    "\n",
    "$$\n",
    "A X=X \\Lambda\n",
    "$$\n",
    "Since the columns of $X$ are linearly independent, $X$ is invertible, and we can write\n",
    "$$\n",
    "A=X \\Lambda X^{-1}\n",
    "$$\n",
    "This is called the eigendecomposition of $A$.\n",
    "\n",
    "\n",
    "Suppose that $A$ is diagonalizable, and let $x_1, \\ldots, x_n$ be linearly independent eigenvectors with corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_n$. Then\n",
    "$$\n",
    "A=X \\Lambda X^{-1}\n",
    "$$\n",
    "where $\\Lambda$ is a diagonal matrix with $\\lambda_i$ on the diagonal, and $X$ is the matrix with the $x_i$ as columns.\n",
    "\n",
    "If $A$ is a square matrix with the eigendecomposition $A=X \\Lambda X^{-1}$,\n",
    "then for any integer $k$, we have \n",
    "\n",
    "$$A^k = X \\Lambda^k X^{-1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in M_n(\\mathbb{F})$. The polynomial \n",
    "\n",
    "$$p_A(x) = \\text{det} (A -xI)$$\n",
    "\n",
    "is called the characteristic polynomial of $A$. The polynomial $P_a(x)$\n",
    "has degree $n$ It has $n$ complex roots (possibly repeated), and each \n",
    "root is an eigenvalue of $A$.\n",
    "\n",
    "If all the eigenvalues of $A$ are distinct, then indeed $A$ has $n$ linearly\n",
    "independent eigenvectors and hence is diagonalizable. However, the situation \n",
    "gets more complicated if $p_A(x)$ has repeated roots. For example, if \n",
    "$p_A(x) = (x-1)^2(x-3)^4$, then we say that $p_A$ has a root at $x=1$ \n",
    "of _multiplicity_ $2$ and a root at $x=3$ of _multiplicity_ 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be a root of $p_A$\n",
    "with multiplicity $k$. Then we say that the __algebraic multiplicity__\n",
    "of $\\lambda$ as an eigenvalue of $A$ is $k$.\n",
    "\n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$.\n",
    "The __eigenspace__ $E_{\\lambda}$ is the set of all vectors $x$ which are\n",
    "eigenvectors of $A$ with eigenvalue $\\lambda$. The dimension of $E_{\\lambda}$\n",
    "is called the __geometric multiplicity__ of $\\lambda$.\n",
    "\n",
    "We say a matrix $A$ is __defective__ if, for one of its eigenvalues, \n",
    "the algebraic multiplicity strictly exceeds the geometric multiplicity. Thus,\n",
    "\"defective\" is just another way of saying \"not diagonalizable\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gershgoin disc theorem \n",
    "\n",
    "Although it is difficult to in general to make simple statements about the \n",
    "eigenvalues of a matrix by looking at its entries, there is one major \n",
    "exception: the Gershgorin disc theorem. \n",
    "\n",
    "Please watch the following video and get a feeling of it. \n",
    "\n",
    "<iframe width=\"600\" height=\"325\" src=\"https://www.youtube.com/embed/rla9Q4E6hVI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{C}^{n \\times n}$. For $1 \\leq i \\leq n$, the __Gershgorin disc__\n",
    "$D_i$ is the disc in the complex plane with the center at $a_{ii}$ and \n",
    "radius $r=\\sum_{j \\neq i}|a_{ij}|$:\n",
    "\n",
    "$$D_i = \\{ z \\in \\mathbb{C} | |z - a_{ii}| \\leq \\sum_{j \\neq i} |a_{ij}| \\}$$\n",
    "\n",
    "With this definition, we can state the following theorem:\n",
    "\n",
    "_All the eigenvalues of $A \\in \\mathbb{C}^{n \\times n}$ are located in one of \n",
    "its Gershgorin disc_. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccc}\n",
    "3 & i & 1 \\\\\n",
    "-1 & 4+5 i & 2 \\\\\n",
    "2 & 1 & -1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "(as above) then the three Gershgorin discs have:\n",
    "- centre 3 and radius $|i|+|1|=2$,\n",
    "- centre $4+5 i$ and radius $|-1|+|2|=3$,\n",
    "- centre $-1$ and radius $|2|+|1|=3$.\n",
    "\n",
    "<img src=\"https://www.maths.ed.ac.uk/~tl/images/discs400.png\" width=\"30%\">\n",
    "\n",
    "Gershgorin’s theorem says that every eigenvalue lies in the union of these three discs. My statement about real and imaginary parts follows immediately.\n",
    "\n",
    "Proof. Consider an eigenvector:\n",
    "\n",
    "$$Ax = \\lambda x $$\n",
    "\n",
    "For instance, we could have \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3  \\\\\n",
    "a & b & c \\\\\n",
    "3 & 2 & 1 \\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix} = \\lambda \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, take the component $x_i$ with the largest magnitude in $x$ (in the above\n",
    "example, $i=3$). Then \n",
    "\n",
    "$$ A_i x = \\sum_j a_{ij} x_j = \\lambda x_i $$\n",
    "\n",
    "In the specific example, we have \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix} = \\lambda x_3\n",
    "$$\n",
    "\n",
    "Now, taking $a_{ii}$ to the other side, we have\n",
    "\n",
    "$$a_{ii} x_i = \\lambda x_i - \\sum_{j \\neq i} a_{ij} x_j$$\n",
    "\n",
    "Divide by $x_i \\neq 0$:\n",
    "\n",
    "$$a_{ii} - \\lambda = - \\sum_{j \\neq i} a_{ij} \\frac{x_j}{x_i}, \\ \\frac{|x_j|}{|x_i|} \\leq 1$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$|\\lambda - a_{ii} | \\leq \\sum_{j \\neq i} |a_{ij}|$$\n",
    "\n",
    "Or \n",
    "\n",
    "$$\n",
    "\\left(\\lambda-a_{i i}\\right) x_i=\\sum_{j \\neq i} a_{i j} x_j .\n",
    "$$\n",
    "Now take the modulus of each side:\n",
    "$$\n",
    "\\left|\\lambda-a_{i i}\\right|\\left|x_i\\right|=\\left|\\sum_{j \\neq i} a_{i j} x_j\\right| \\leq \\sum_{j \\neq i}\\left|a_{i j}\\right|\\left|x_j\\right| \\leq\\left(\\sum_{j \\neq i}\\left|a_{i j}\\right|\\right)\\left|x_i\\right|=r_i\\left|x_i\\right|\n",
    "$$\n",
    "where to get the inequalities, we used the triangle inequality and then the maximal property of $\\left|x_i\\right|$. Cancelling $\\left|x_i\\right|$ gives $\\left|\\lambda-a_{i i}\\right| \\leq r_i$. And that's it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"600\" height=\"325\" src=\"https://www.youtube.com/embed/19FXch2X7sQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is an eigenvalue of $A$ (which means $A$ is a square matrix), \n",
    "show that it is an eigenvalue of $A^T$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{det}(A-\\lambda I) & = 0 \\\\ \n",
    "\\text{det}((A-\\lambda I)) & = \\text{det}((A-\\lambda I)^T) \\\\\n",
    "\\text{det}(A^T - \\lambda I) & = 0 \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the eigenvalues of $A$ and $A^T$ are the same, we also  have the \n",
    "eigenvalues of $A$ lie within the Gershgorin discs of $A^T$.\n",
    "\n",
    "This  theorem shows that if the off-diagonal entries of a matrix $A$ \n",
    "are small, the eigenvalues cannot be too far from the diagonal entries of \n",
    "$A$. Therefore, if we have an algorithm that reduces the magnitude of \n",
    "the off-diagonal entries, it can be used to approximate the eigenvalues \n",
    "of $A$. Of course, diagonal entries will change in the process of minimizing \n",
    "the off-diagonal entries. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what happens at the off-diagonal entries get larger and larger, \n",
    "consider the matrix:\n",
    "\n",
    "$$C(t) = (1-t)D + tA$$\n",
    "\n",
    "where $D$ is the diagonal of $A$, and $t$ is a parameter between $0$ and $1$.\n",
    "When $t = 0, C(0) = D$ and all the Gershgorin discs are centered at the \n",
    "diagonal entries $a_{ii}$ and have radius zero. When $t=1, C(1)$ is simply \n",
    "$A$. \n",
    "\n",
    "As $t$ increases from $0$ to $1$, it turns out that the radius of the Gershgorin\n",
    "discs increase linearly with $t$, from $0$ to \n",
    "\n",
    "$$\\sum_{j \\neq i} |a_{ij}|$$\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/gershgorin.png\" width=\"79%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the formula,\n",
    "\n",
    "$$C(t) = (1-t)D + tA$$\n",
    "\n",
    "when $t$ is small, all of the discs are still disjoint, each disc contains \n",
    "exactly one eigenvalue, which is close to the corresponding eigenvalue of \n",
    "$A$. As $t$ grows, it could happen that two discs merge. In this case,\n",
    "something special happens. After the merge, the union of both two disc\n",
    "contains exactly two eigenvalues (possibly with algebraic multiplicity 2).\n",
    "These two eigenvalues may lie in both discs or only one of them: one eigenvalue\n",
    "may move from one disc to another after the discs merge. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitarily diagonalizable matrices \n",
    "\n",
    "The eigendecomposition is very useful, but - as we will see later - it can \n",
    "become difficult to compute with when $X$ is ill-conditioned (that is,\n",
    "if the eigenvectors are nearly linearly dependent). one reason is that \n",
    "computing $X^{-1}$ becomes difficult and can lead to large numerical errors. \n",
    "\n",
    "Thus, we might ask ourselves when $X$ is guaranteed to be _well-conditioned_.\n",
    "In that case, $X$ would be orthogonal or unitary. \n",
    "\n",
    "With this in mind, we say that a matrix $A \\in C^{n \\times n}$ is\n",
    "__unitarily diagonalizable__ if there is a unitary matrix $Q$ and \n",
    "a diagonal matrix $\\Lambda$ so that \n",
    "\n",
    "$$A = Q \\Lambda Q^H$$\n",
    "\n",
    "A matrix $A \\in C^{n}$ is unitarily diagonalizable if and only if \n",
    "\n",
    "$$A^HA = AA^H$$\n",
    "\n",
    "If this condition holds, we say that $A$ is __normal__."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $A$ is complex Hermitian, that is, $A^H = A$, then the eigenvalues of \n",
    "$A$ are real, and \n",
    "\n",
    "$$A = Q \\Lambda Q^H$$\n",
    "\n",
    "With $\\Lambda$ real and $Q$ unitary. If $A$ is real symmetric, then in \n",
    "addition we have that $Q$ is real orthogonal, and \n",
    "\n",
    "$$A = Q \\Lambda Q^T$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jordan form \n",
    "\n",
    "What happens when a square $n \\times n$ matrix $A$ is not diagonalizable?\n",
    "We know that $A$ still has $n$ eigenvalues (counting multiplicities) because\n",
    "the characteristic polynomial $p_A$ has $n$ complex roots. It turns out \n",
    "that even if we don't have a full basis of eigenvectors, we can \n",
    "still use the eigenvalues, and the eigenvectors we do have, to decompose\n",
    "$A$ in a useful way. This is called the _jordan canonical form_.\n",
    "\n",
    "Before we define the Jordan form, we define a Jordan block. A Jordan block is either a matrix of size 1 or of size greater than 1 ; if greater than 1 , it is a matrix of the form\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccccc}\n",
    "\\lambda & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda & 1 \\\\\n",
    "0 & 0 & \\cdots & 0 & \\lambda\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "with ones on the super-diagonal (that is, ones in the entries $J_{i, i+1}$ ).\n",
    "All square matrices can be written in the Jordan canonical form:\n",
    "\n",
    "For any matrix $A \\in \\mathbb{C}^{n \\times n}$, there exist $X$ and $J$ such that\n",
    "$$\n",
    "A=X J X^{-1} \\text {. }\n",
    "$$\n",
    "The matrix $J$ is a block diagonal matrix,\n",
    "$$\n",
    "J=\\left(\\begin{array}{lll}\n",
    "J_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & J_k\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "with Jordan blocks $J_k$ on the diagonal.\n",
    "\n",
    "Each block $J_k$ corresponds to an eigenvalue $\\lambda$ of $A$, which appears on its diagonal. If an eigenvalue $\\lambda$ has geometric multiplicity $g$, then there are $g$ distinct Jordan blocks with $\\lambda$ on the diagonal. Thus, the Jordan form can be seen as a generalization of the eigendecomposition: if the algebraic multiplicity is equal to the geometric multiplicity for each eigenvalue, then each Jordan block has size 1. In this case, $J$ is diagonal, and we recover the eigendecomposition.\n",
    "\n",
    "We will see several algorithms later in this book for computing eigendecompositions accurately. However, the Jordan decomposition is very difficult to compute accurately on a computer. To see why, suppose we have a matrix $A$ which has an eigenvalue $\\lambda$ with algebraic multiplicity 2 and geometric multiplicity 1 . It turns out that even a small change in the matrix could turn this single eigenvalue into two distinct eigenvalues. This wouldn't"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur decomposition \n",
    "\n",
    "We have seen that if $A$ is a normal matrix (a very strong property!), then \n",
    "it is unitarily diagonalizable: $A = Q \\Lambda Q^H$ for some unitary $Q$.\n",
    "When $A$ is an arbitrary matrix, all we get is the Jordan canonical form :\n",
    "$A = XJX^{-1}$. The Jordan canonical form is more difficult to work \n",
    "with, both because $X$ might be poorly conditioned and because this \n",
    "decomposition isn't very robust to small errors. \n",
    "\n",
    "The _Schur decomposition_ gets (almost) the benefits of unitary diagonalization\n",
    "but for arbitrary matrices. \n",
    "\n",
    "For any complex matrix $A \\in \\mathbb{C}^{n \\times n}$, there is a unitary \n",
    "matrix $Q$ and an upper triangular matrix $T$ so that \n",
    "\n",
    "$$A = Q T Q^H$$\n",
    "\n",
    "This is called the __Schur decomposition__. The eigenvalues of $A$ \n",
    "appear along the diagonal of $T$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $2 \\times 2$ __block upper-triangular matrix__ has the form where all of \n",
    "the blocks on the diagonal are either $2 \\times 2$ or $1 \\times 1$. \n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/block-matrix.webp\" alt=\"block-matrix\" width=\"20%\">\n",
    "\n",
    "For any real matrix $A \\in \\mathbb{R}^{n \\times n}$, there is a real \n",
    "orthogonal matrix $Q$ and a $2 \\times 2$ block upper-triangular matrix\n",
    "$T$ so that \n",
    "\n",
    "$$A = Q T Q^T$$\n",
    "\n",
    "The $2 \\times 2$ blocks on the diagonal of $T$ contain two complex conjugate\n",
    "eigenvalues of $A$, and the $1 \\times 1$ blocks contain a single real eigenvalue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decompositions we've seen so far - the eigendecomposition, Jordan form, \n",
    "and (real) Schur decomposition - are all __eigenvalue revealing__ in the \n",
    "sense that the eigenvalues of the matrix $A$ lie on the diagonal of \n",
    "some matrix involved in the decomposition. To summarize what we have seen:\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/matrix-decomposition.webp\" alt=\"matrix-decomposition\" width=\"70%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition \n",
    "\n",
    "In the previous section we saw several matrix decompositions based on eigenvalues. Now we'll see another decomposition based on singular values instead of eigenvalues. This is called the singular value decomposition (SVD), and it is an extremely important tool for understanding and computing with matrices.\n",
    "\n",
    "To motivate the SVD, consider the action of a matrix $A$ on the unit sphere. It turns out that $A$ maps the sphere to some hyperellipsoid $E$ (that is, a high-dimensional ellipsoid). Let's define the following quantities:\n",
    "\n",
    "- The lengths of the semi-axes of $E$ are denoted $\\sigma_{1}, \\ldots, \\sigma_{n}$. These are called the singular values of $A$. By convention, they are ordered so that $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}$.\n",
    "\n",
    "- The directions of the semi-axes are denoted by unit vectors $u_{1}, \\ldots, u_{n}$ (so that the $i$ th semi-axis is $\\sigma_{i} u_{i}$ ). The vectors $u_{i}$ are called the __left singular vectors of__ $A$. \n",
    "\n",
    "- For each $u_{i}$, there is some unit vector $v_{i}$ so that $A v_{i}=\\sigma_{i} u_{i}$. The vectors $v_{i}$ are called the __right singular vectors__.\n",
    "\n",
    "Here's a picture of these quantities for some $2 \\times 2$ matrix $A$ :\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/svd-illustration1.webp\" alt=\"svd-illustration1\" width=\"70%\">\n",
    "\n",
    "Notice that $A$ doesn't have to be a square matrix for the definitions above to make sense. The singular vectors and singular values can be used to form the singular value decomposition:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider any matrix $A \\in \\mathbb{C}^{m \\times n}$ and $p=\\min (m, n)$. There exist two unitary matrices $U \\in \\mathbb{C}^{m \\times m}$ and $V \\in \\mathbb{C}^{n \\times n}$ and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with real non-negative entries such that\n",
    "\n",
    "$$\n",
    "A=U \\Sigma V^{H} \\text {. }\n",
    "$$\n",
    "\n",
    "If $m>n, \\Sigma$ has zeros in rows $p+1$ to $m$. If $n>m, \\Sigma$ has zeros in columns $p+1$ to $n$. This is called the singular value decomposition.\n",
    "\n",
    "The diagonal entries of $\\Sigma$ are $\\sigma_{1}, \\ldots, \\sigma_{p}$, and are called the singular values. By convention we order the singular values so that $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{p} \\geq 0$. The columns of $U$ are called the __left singular vectors__, and the columns of $V$ are called the __right singular vectors__.\n",
    "\n",
    "If $A$ is real, then $U$ and $V$ are real orthogonal.\n",
    "\n",
    "The singular values are uniquely determined. In general, $U$ and $V$ are not unique. However, if all the singular values are distinct and ordered from large to small, $U$ and $V$ are uniquely determined (up to the sign of their columns if they are real, or a multiplication by $e^{\\imath \\theta}$ if they are complex). If $A$ has rank $r$, then $\\sigma_{i}=0$ for $r<i \\leq p$. In other words, the number of non-zero singular values is equal to the rank of the matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why the decomposition in the theorem above lines up with the geometric intuitions, consider a matrix\n",
    "\n",
    "$$\n",
    "A=U \\Sigma V^{H} \\text {. }\n",
    "$$\n",
    "\n",
    "We can view the action of $A$ on the unit sphere as a composition of three linear maps, $V^{H}$, $\\Sigma$, and $U$. Since $U$ and $V$ are unitary, their actions are rotations and reflections. Since $\\Sigma$ is diagonal, all it does is re-scale the coordinate axes. As a result, we can represent the action of $A$ as follows:\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/svd-illustration2.webp\" alt=\"svd-illustration2\" width=\"70%\">\n",
    "\n",
    "Since $U$ and $V$ are just rotations or reflections, the singular values $\\sigma_{1}, \\ldots, \\sigma_{n}$ captures anything that $A$ does to the 2-norm of vectors. In particular, both the 2-norm of $A$ and the __Frobenius norm__ of $A$ can be written in terms of the singular values:\n",
    "\n",
    "For $A \\in \\mathbb{R}^{m \\times n}$ :\n",
    "\n",
    "$$\n",
    "\\|A\\|_{2}=\\sigma_{1}(A),\\|A\\|_{F}=\\sqrt{\\sum_{i=1}^{\\min \\{m, n\\}} \\sigma_{i}^{2},}\n",
    "$$\n",
    "\n",
    "where $\\sigma_{1}$ is the largest singular value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition number and the SVD\n",
    "\n",
    "For a non-singular $n \\times n$ matrix $A$, the __condition number__ of $A$, denoted $\\kappa(A)$, is defined as\n",
    "\n",
    "$$\n",
    "\\kappa(A)=\\|A\\|_{2}\\left\\|A^{-1}\\right\\|_{2} .\n",
    "$$\n",
    "\n",
    "The condition number is also given by\n",
    "\n",
    "$$\n",
    "\\kappa(A)=\\frac{\\sigma_{1}}{\\sigma_{n}} .\n",
    "$$\n",
    "\n",
    "As we will see later on in Chapter 3 , the condition number of a matrix is a key factor when determining the accuracy of a numerical calculation involving $A$, for example, when solving a linear system $A x=b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular values vs. eigenvalues \n",
    "\n",
    "When $A$ is a symmetric matrix, the singular values and the eigenvalues are the same, up to a sign:\n",
    "\n",
    "\n",
    "For a symmetric $n \\times n$ matrix $A$ with eigenvalues $\\lambda_{i}$ and singular values $\\sigma_{i}$ so that $\\left|\\lambda_{1}\\right| \\geq\\left|\\lambda_{2}\\right| \\geq \\cdots \\geq\\left|\\lambda_{n}\\right|$ and $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}$,\n",
    "\n",
    "$$\n",
    "\\sigma_{i}=\\left|\\lambda_{i}\\right|\n",
    "$$\n",
    "\n",
    "for all $i$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when $A$ is not symmetric, the singular values and the eigenvalues can be very different. As discussed above, the eigenvalues capture how $A^{k}$ behave for large $k$, while the singular values capture the geometry of $A$ as a linear transformation. As an example of how these can be different, consider the matrix\n",
    "\n",
    "$$\n",
    "A=X\\left(\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right) X^{-1}\n",
    "$$\n",
    "\n",
    "for some invertible matrix $X$.\n",
    "\n",
    "It is clear that the eigenvalues of $A$ are $\\pm 1$. Further, you can check that $A^{2}=I$, so $A^{k}$ is equal to $I$ if $k$ is even and equal to $A$ if $k$ is odd. Thus, the behavior of $A^{k}$ is bounded as $k$ grows in the sense that it just oscillates between two matrices $I$ and $A$; it does not blow up with $k$.\n",
    "\n",
    "On the other hand, the singular values of $A$ might get quite large. If $X$ is orthogonal, then $\\sigma_{1}=\\sigma_{2}=1$. But as $X$ gets further and further from orthogonal, the singular values of $A$ can become larger and larger. To see why, we can imagine multiplying by $A$ one factor at a time and seeing what happens to the unit circle. Multiplying by $X^{-1}$ \"stretches\" the unit circle out. Then the matrix $\\left(\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right)$ flips the space over the $y$-axis. Finally, $X$ \"undoes\" the stretching that $X^{-1}$ did, but because of the flip, things don't get put back where they came from, and some points remain stretched. For example, the situation might look like this:\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/svd-illustration3.webp\" alt=\"svd-illustration3\" width=\"70%\">\n",
    "\n",
    "In this case, the unit ball gets mapped to some stretched-out ellipsoid, and there is some point on the unit ball which gets mapped to a much longer vector. This means that $\\sigma_{1}$ is large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even if $A$ is not symmetric, $A^{T} A$ and $A A^{T}$ are symmetric, and it turns out that the eigenvalues of these matrices are related to the singular values of $A$.\n",
    "\n",
    "Let $A$ be a square matrix of size $n$ with singular values $\\sigma_{i}$. Then both $A^{T} A$ and $A A^{T}$ have eigenvalues $\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}$. Moreover, the right singular vectors are the eigenvectors of $A^{T} A$, while the left singular vectors are the eigenvectors of $A A^{T}$ :\n",
    "\n",
    "$$\n",
    "A^{T} A v_{i}=\\sigma_{i}^{2} v_{i}, A A^{T} u_{i}=\\sigma_{i}^{2} u_{i} .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different shapes of SVD \n",
    "\n",
    "For square matrices, the three factors $U, \\Sigma$, and $V$ have the same shape. For rectangular matrices, we can work two different decompositions: the full SVD (the one defined above) and the thin SVD. \n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/svd-full.webp\" alt=\"svd-full\" width=\"50%\">\n",
    "\n",
    "However, all of those zeros in $\\Sigma$ make parts of either $U$ or $V^{H}$ extraneous. By trimming off these extra parts, we end up with the thin SVD:\n",
    "\n",
    "For any matrix $A \\in \\mathbb{C}^{m \\times n}$ there are matrices $\\hat{U}$ and $\\hat{V}$ so that $\\hat{U}^{H} \\hat{U}=I$ and $\\hat{V}^{H} \\hat{V}=I$, and a diagonal matrix $\\hat{\\Sigma}$ so that\n",
    "\n",
    "$$\n",
    "A=\\hat{U} \\hat{\\Sigma} \\hat{V}^{H} .\n",
    "$$\n",
    "\n",
    "Let $p=\\min (m, n)$. Then $\\hat{\\Sigma}$ is $p \\times p, \\hat{U}$ is $m \\times p$, and $\\hat{V}$ is $n \\times p$.\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/svd-thin.webp\" alt=\"svd-thin\" width=\"50%\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's it!__ Now you know all the linear algebra you need for implementing\n",
    "all those famous algorithms we will learn in this series of posts, such as:\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/top-10.webp\" alt=\"top-10-algorithms\" width=\"70%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. [In Praise of the Gershgorin Disc Theorem](https://golem.ph.utexas.edu/category/2016/08/in_praise_of_the_gershgorin_di.html)\n",
    "2. [Gershgorin circle theorem](https://www.geogebra.org/m/wDEj3Xg9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (default, Nov 23 2021, 15:27:38) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe189d182e892b13e31889708ff6fa1238858c117c3508e6a10c00d2dc805a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
