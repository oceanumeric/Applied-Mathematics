{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Bootcamp\n",
    "\n",
    "This chapter provides a quick overview of linear algebra, focusing on the\n",
    "mathematical ideas and definitions that are foundational for the rest \n",
    "of the book. Many key concepts are omitted from this chapter as it assumes \n",
    "readers should have some basic knowledge about linear algebra. For instance,\n",
    "you should know the definition of _linearly independent_, _basis_ and\n",
    "_dot product_, etc. \n",
    "\n",
    "The angle $\\theta$ between two vectors $x, y \\in $\\mathbb{R}^n$ satisfies\n",
    "\n",
    "$$x^T y = ||x||_2 ||y||_2 \\cos \\theta $$\n",
    "\n",
    "or in a less rigorous way of expressing\n",
    "\n",
    "$$\\cos \\theta = \\frac{x^T y }{||x|| ||y||}$$\n",
    "\n",
    "A set of vectors $x_1, \\cdots, x_k$ are __orthogonal vectors__ if they are\n",
    "pairwise orthogonal, that is, if $x_i^T x_j = 0$ for all $i \\neq j$. For \n",
    "instance, if $x$ and $y$ are orthogonal, then \n",
    "\n",
    "$$||x+y||_2^2 = ||x||^2 + ||y||^2$$\n",
    "\n",
    "For the subspace $S$ of $\\mathbb{R}^n$, we define the __orthogonal complement__\n",
    "of $S$, denoted $S^{\\perp}$, to be the set of vectors in $\\mathbb{R}^n$ which\n",
    "are orthogonal to $S$.\n",
    "\n",
    "This means we have \n",
    "\n",
    "$$\\text{dim}(S) + \\text{dim}(S^{\\perp}) = n $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the __Cauchy-Schwarz__ inequality (and the more general __Hölder's inequality__)\n",
    "to bound a dot product of vectors by the norms of the vectors. \n",
    "\n",
    "Hölder's inequality says that for any vectors $x, y$ and positive numbers\n",
    "$p, q$ so that $1/p + 1/q = 1$, \n",
    "\n",
    "$$|x^Ty| \\leq ||x||_p ||y||_q$$\n",
    "\n",
    "When specialized to $p = q = 2$. We get the __Cauchy-Schwarz inequality__. For\n",
    "any vectors $x$ and $y$, \n",
    "\n",
    "$$|x^Ty| \\leq ||x||_2 ||y||_2 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complex matrix, we define the __conjugate transpose__ of $A$, denoted \n",
    "$A^H$, to be the matrix that we get when we take the entry-wise complex conjugate\n",
    "of $A^T$. That is, the $(i, j)$ entry of $A^H$ is $\\overline{a_{ji}}$\n",
    "\n",
    "Suppose we want to calculate the conjugate transpose of the \n",
    "following matrix $\\boldsymbol{A}$.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\\left[\\begin{array}{ccc}\n",
    "1 & -2-i & 5 \\\\\n",
    "1+i & i & 4-2 i\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "We first transpose the matrix:\n",
    "$$\n",
    "\\boldsymbol{A}^{\\top}=\\left[\\begin{array}{cc}\n",
    "1 & 1+i \\\\\n",
    "-2-i & i \\\\\n",
    "5 & 4-2 i\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Then we conjugate every entry of the matrix:\n",
    "$$\n",
    "\\boldsymbol{A}^{\\mathrm{H}}=\\left[\\begin{array}{cc}\n",
    "1 & 1-i \\\\\n",
    "-2+i & -i \\\\\n",
    "5 & 4+2 i\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __symmetric__ matrix is such t hat \n",
    "\n",
    "$$A^T = A$$\n",
    "\n",
    "A __skew-symmetric matrix__ is such that \n",
    "\n",
    "$$A^T = - A$$\n",
    "\n",
    "A __Hermitian__ matrix is such that \n",
    "\n",
    "$$A^H = A$$\n",
    "\n",
    "A __skew-Hermitian__ matrix has \n",
    "\n",
    "$$A^H = -A$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra of matrix\n",
    "\n",
    "$$(AB)C = A(BC)$$\n",
    "\n",
    "$$(AB)^T = B^T A^T$$\n",
    "\n",
    "$$(A \\pm B)^T = A^T \\pm B^T$$\n",
    "\n",
    "$$A^{-1} A = AA^{-1} = I $$\n",
    "\n",
    "If $A$ is square and not invertible, we say that it is __singular__. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sherman-Morrison-Woodbury formula\n",
    "\n",
    "We will learn different ways to calculate matrix inverse later, but here is\n",
    "one useful tool that you can think about now. \n",
    "\n",
    "If $A \\in \\mathbb{R}^{n \\times n}$, and $U, V \\in \\mathbb{R}^{n \\times k}$:\n",
    "\n",
    "$$(A + UV^T)^{-1} = A^{-1} - A^{-1} U(I + V^TA^{-1}U)^{-1} V^T A^{-1}$$\n",
    "\n",
    "where we assume that $A$ and $I + V^T A^{-1} U$ are non-singular."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix norms \n",
    "\n",
    "The Frobenius norm of $A$ is defined by\n",
    "$$\n",
    "\\|A\\|_F=\\left(\\sum_{i j} a_{i j}^2\\right)^{1 / 2}=\\sqrt{\\operatorname{tr}\\left(A A^H\\right)},\n",
    "$$\n",
    "where $A^H$ is the transpose conjugate matrix.\n",
    "\n",
    "The trace of a square matrix $A$ is defined as $\\text{tr}(A) = \\sum_i a_{ii}$\n",
    "\n",
    "For any invertible matrix $P$,\n",
    "\n",
    "$$\\text{tr}(A) = \\text{tr}(P^{-1}AP)$$\n",
    "\n",
    "We also have \n",
    "\n",
    "$$\\text{tr}(AB) = \\text{tr}(BA)$$\n",
    "\n",
    "$$\\text{tr}(A(BC)) = \\text{tr}((BC)A)$$\n",
    "\n",
    "$$\\text{tr}(A + \\alpha B) = \\text{tr}(A) + \\alpha \\ \\text{tr}(B)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The determinant of a matrix \n",
    "\n",
    "The determinant of matrix $A$ is equal to\n",
    "$$\n",
    "\\operatorname{det}(A)=\\sum_{\\sigma \\in S_n} \\operatorname{sign}(\\sigma) \\prod_{i=1}^n a_{i, \\sigma_i},\n",
    "$$\n",
    "where $S_n$ is the set of all permutations of $\\{1, \\ldots, n\\}$, and $\\operatorname{sign}(\\sigma)$ is the signature of the permutation $\\sigma$. Here, the signature of a permutation $\\sigma$ is $+1$ if $\\sigma$ can be realized by doing an even number of pairwise swaps, and it is $-1$ otherwise; it turns out (though it's not obvious!) that this is well-defined.\n",
    "\n",
    "The determinant satisfies the following three fundamental properties:\n",
    "1. $\\operatorname{det}\\left(I_n\\right)=1$.\n",
    "\n",
    "2. The determinant is an $n$-linear function. That is, if we \"fix\" all the columns \n",
    "of $A$ except column $i, \\operatorname{det}(A)$ is a linear function of $a_i$.\n",
    "\n",
    "3. The determinant is an alternating form. That is, when two columns of $A$ are identical, then $\\operatorname{det}(A)=0$.\n",
    "\n",
    "In fact, these three properties uniquely define the determinant \n",
    "(that is, the determinant is the only function satisfying these three properties).\n",
    "\n",
    "1. For any square matrix $A$ and scalar $\\alpha \\in \\mathbb{R}$, $\\operatorname{det}(\\alpha A)=\\alpha^n \\operatorname{det}(A)$.\n",
    "2. For any square matrices $A$ and $B, \\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B)$.\n",
    "3. A square matrix $A$ is singular (that is, not invertible) if and only if $\\operatorname{det}(A)=0$.\n",
    "4. $\\operatorname{det}(A)=\\operatorname{det}\\left(A^T\\right)$.\n",
    "5. $\\operatorname{det}\\left(A^{-1}\\right)=\\operatorname{det}(A)^{-1}$.\n",
    "\n",
    "For any square matrix $A$:\n",
    "\n",
    "$$\\operatorname{det}(\\text{exp}A) = \\text{exp}(\\text{tr} A)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal matrices and projectors \n",
    "\n",
    "An orthogonal matrix $Q \\in \\mathbb{R}^{m \\times n}$ is a matrix whose columns are orthonormal. That is, the columns $q_1, \\ldots, q_n$ of $Q$ satisfy\n",
    "$$\n",
    "q_i^T q_j=\\delta_{i j} .\n",
    "$$\n",
    "Equivalently,\n",
    "$$\n",
    "Q^T Q=I\n",
    "$$\n",
    "\n",
    "A __unitary matrxi__ $U \\in \\mathbb{C}^{n \\times n}$ is a matrix so that\n",
    "\n",
    "$$U^HU = UU^H = I $$\n",
    "\n",
    "Multiplication by an orthogonal matrix $Q$ preserves 2-norms:\n",
    "\n",
    "$$||Qx||_2^2 = x^T Q^T Q x = x^Tx = ||x||_2^2 $$\n",
    "\n",
    "__Square orthogonal__ matrices are always invertible by definition, and \n",
    "\n",
    "$$Q^{-1} = Q^{T}$$\n",
    "\n",
    "As a result, for square matrices, we have \n",
    "\n",
    "$$\\text{det}(Q) = \\pm 1$$\n",
    "\n",
    "\n",
    "Note that if $Q$ is square orthogonal, then \n",
    "\n",
    "$$Q^TQ = QQ^T = I$$\n",
    "\n",
    "If $Q$ is rectangular orthogonal, we only have \n",
    "\n",
    "$$Q^TQ = I$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the vector of response values is denoted by $y$ and the vector of \n",
    "fitted values by $\\hat{y}$\n",
    "\n",
    "$$\\hat{y} = Py$$\n",
    "\n",
    "The vector of residuals $r$ can also be expressed compactly using the \n",
    "__projection matrix__:\n",
    "\n",
    "$$r = y-\\hat{y} = y - Py = (I-P)y$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues, eigenvectors, and related matrix decompositions \n",
    "\n",
    "In this section, we will introduce __eigenvalues__ and __eigenvectors__,\n",
    "fundamental tools in linear algebra. Using these, we will define a number\n",
    "of different __matrix decompositions__, that is, ways to decompose a \n",
    "matrix as a product of easy-to-understand matrices. Later you will realize\n",
    "that matrix decompositions are an essential part of numerical linear\n",
    "algebra. \n",
    "\n",
    "Easy-to-understand matrices include:\n",
    "\n",
    "- __Diagonal matrices__, which have non-zero entries only on the diagonal. \n",
    "These are very easy to work with!\n",
    "\n",
    "- __Orthogonal matrices__, which are easier to invert (take the transpose!).\n",
    "As we will see, orthogonal matrices are also less prone to inaccuracies in \n",
    "computation. \n",
    "\n",
    "- __Triangular matrices__, which can be either __lower-triangular__ \n",
    "(with non-zero entries on or below the diagonal only) or __upper-triangular__\n",
    "(non-zero entries on or above the diagonal). As we will see, it can be more \n",
    "computationally efficient to deal with triangular matrices than with \n",
    "general matrices. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{C}^{n \\times n}$ be a __square__ matrix, and let \n",
    "$x \\in \\mathbb{C}^n$ be non-zero and $\\lambda \\in \\mathbb{C}$.\n",
    "We say that $x$ is an __eigenvector__ of $A$ with __eigenvalue__ $\\lambda$\n",
    "if \n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "\n",
    "The determinant and trace can be expressed in terms of the eigenvalues:\n",
    "\n",
    "$$\\text{det}(A) = \\prod_{i=1}^n \\lambda_i, \\ \\text{tr}(A) = \\sum_{i=1}^n \\lambda_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrices $A$ have $n$ linearly independent eigenvectors $x_1, \\ldots, x_n$. If this is the case, we say that $A$ is diagonalizable. The reason for the name is that if $A$ has $n$ linearly independent eigenvectors, we can decompose it as follows. Say that $\\lambda_1, \\ldots, \\lambda_n$ are the corresponding eigenvalues, and let $\\Lambda$ be the diagonal matrix whose $i$ th diagonal entry is $\\lambda_i$, and let $X$ be the matrix with the $x_i$ as columns. In matrix notation,\n",
    "\n",
    "$$\n",
    "A X=X \\Lambda\n",
    "$$\n",
    "Since the columns of $X$ are linearly independent, $X$ is invertible, and we can write\n",
    "$$\n",
    "A=X \\Lambda X^{-1}\n",
    "$$\n",
    "This is called the eigendecomposition of $A$.\n",
    "\n",
    "\n",
    "Suppose that $A$ is diagonalizable, and let $x_1, \\ldots, x_n$ be linearly independent eigenvectors with corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_n$. Then\n",
    "$$\n",
    "A=X \\Lambda X^{-1}\n",
    "$$\n",
    "where $\\Lambda$ is a diagonal matrix with $\\lambda_i$ on the diagonal, and $X$ is the matrix with the $x_i$ as columns.\n",
    "\n",
    "If $A$ is a square matrix with the eigendecomposition $A=X \\Lambda X^{-1}$,\n",
    "then for any integer $k$, we have \n",
    "\n",
    "$$A^k = X \\Lambda^k X^{-1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in M_n(\\mathbb{F})$. The polynomial \n",
    "\n",
    "$$p_A(x) = \\text{det} (A -xI)$$\n",
    "\n",
    "is called the characteristic polynomial of $A$. The polynomial $P_a(x)$\n",
    "has degree $n$ It has $n$ complex roots (possibly repeated), and each \n",
    "root is an eigenvalue of $A$.\n",
    "\n",
    "If all the eigenvalues of $A$ are distinct, then indeed $A$ has $n$ linearly\n",
    "independent eigenvectors and hence is diagonalizable. However, the situation \n",
    "gets more complicated if $p_A(x)$ has repeated roots. For example, if \n",
    "$p_A(x) = (x-1)^2(x-3)^4$, then we say that $p_A$ has a root at $x=1$ \n",
    "of _multiplicity_ $2$ and a root at $x=3$ of _multiplicity_ 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be a root of $p_A$\n",
    "with multiplicity $k$. Then we say that the __algebraic multiplicity__\n",
    "of $\\lambda$ as an eigenvalue of $A$ is $k$.\n",
    "\n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$.\n",
    "The __eigenspace__ $E_{\\lambda}$ is the set of all vectors $x$ which are\n",
    "eigenvectors of $A$ with eigenvalue $\\lambda$. The dimension of $E_{\\lambda}$\n",
    "is called the __geometric multiplicity__ of $\\lambda$.\n",
    "\n",
    "We say a matrix $A$ is __defective__ if, for one of its eigenvalues, \n",
    "the algebraic multiplicity strictly exceeds the geometric multiplicity. Thus,\n",
    "\"defective\" is just another way of saying \"not diagonalizable\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gershgoin disc theorem \n",
    "\n",
    "Although it is difficult to in general to make simple statements about the \n",
    "eigenvalues of a matrix by looking at its entries, there is one major \n",
    "exception: the Gershgorin disc theorem. \n",
    "\n",
    "Please watch the following video and get a feeling of it. \n",
    "\n",
    "<iframe width=\"600\" height=\"325\" src=\"https://www.youtube.com/embed/rla9Q4E6hVI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{C}^{n \\times n}$. For $1 \\leq i \\leq n$, the __Gershgorin disc__\n",
    "$D_i$ is the disc in the complex plane with the center at $a_{ii}$ and \n",
    "radius $r=\\sum_{j \\neq i}|a_{ij}|$:\n",
    "\n",
    "$$D_i = \\{ z \\in \\mathbb{C} | |z - a_{ii}| \\leq \\sum_{j \\neq i} |a_{ij}| \\}$$\n",
    "\n",
    "With this definition, we can state the following theorem:\n",
    "\n",
    "_All the eigenvalues of $A \\in \\mathbb{C}^{n \\times n}$ are located in one of \n",
    "its Gershgorin disc_. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if\n",
    "$$\n",
    "A=\\left(\\begin{array}{ccc}\n",
    "3 & i & 1 \\\\\n",
    "-1 & 4+5 i & 2 \\\\\n",
    "2 & 1 & -1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "(as above) then the three Gershgorin discs have:\n",
    "- centre 3 and radius $|i|+|1|=2$,\n",
    "- centre $4+5 i$ and radius $|-1|+|2|=3$,\n",
    "- centre $-1$ and radius $|2|+|1|=3$.\n",
    "\n",
    "<img src=\"https://www.maths.ed.ac.uk/~tl/images/discs400.png\">\n",
    "\n",
    "Gershgorin’s theorem says that every eigenvalue lies in the union of these three discs. My statement about real and imaginary parts follows immediately.\n",
    "\n",
    "Proof. Consider an eigenvector:\n",
    "\n",
    "$$Ax = \\lambda x $$\n",
    "\n",
    "For instance, we could have \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3  \\\\\n",
    "a & b & c \\\\\n",
    "3 & 2 & 1 \\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix} = \\lambda \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, take the component $x_i$ with the largest magnitude in $x$ (in the above\n",
    "example, $i=3$). Then \n",
    "\n",
    "$$ A_i x = \\sum_j a_{ij} x_j = \\lambda x_i $$\n",
    "\n",
    "In the specific example, we have \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\end{bmatrix} \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\end{bmatrix} = \\lambda x_3\n",
    "$$\n",
    "\n",
    "Now, taking $a_{ii}$ to the other side, we have\n",
    "\n",
    "$$a_{ii} x_i = \\lambda x_i - \\sum_{j \\neq i} a_{ij} x_j$$\n",
    "\n",
    "Divide by $x_i \\neq 0$:\n",
    "\n",
    "$$a_{ii} - \\lambda = - \\sum_{j \\neq i} a_{ij} \\frac{x_j}{x_i}, \\ \\frac{|x_j|}{|x_i|} \\leq 1$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$|\\lambda - a_{ii} | \\leq \\sum_{j \\neq i} |a_{ij}|$$\n",
    "\n",
    "Or \n",
    "\n",
    "$$\n",
    "\\left(\\lambda-a_{i i}\\right) x_i=\\sum_{j \\neq i} a_{i j} x_j .\n",
    "$$\n",
    "Now take the modulus of each side:\n",
    "$$\n",
    "\\left|\\lambda-a_{i i}\\right|\\left|x_i\\right|=\\left|\\sum_{j \\neq i} a_{i j} x_j\\right| \\leq \\sum_{j \\neq i}\\left|a_{i j}\\right|\\left|x_j\\right| \\leq\\left(\\sum_{j \\neq i}\\left|a_{i j}\\right|\\right)\\left|x_i\\right|=r_i\\left|x_i\\right|\n",
    "$$\n",
    "where to get the inequalities, we used the triangle inequality and then the maximal property of $\\left|x_i\\right|$. Cancelling $\\left|x_i\\right|$ gives $\\left|\\lambda-a_{i i}\\right| \\leq r_i$. And that's it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"600\" height=\"325\" src=\"https://www.youtube.com/embed/19FXch2X7sQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is an eigenvalue of $A$ (which means $A$ is a square matrix), \n",
    "show that it is an eigenvalue of $A^T$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{det}(A-\\lambda I) & = 0 \\\\ \n",
    "\\text{det}((A-\\lambda I)) & = \\text{det}((A-\\lambda I)^T) \\\\\n",
    "\\text{det}(A^T - \\lambda I) & = 0 \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the eigenvalues of $A$ and $A^T$ are the same, we also  have the \n",
    "eigenvalues of $A$ lie within the Gershgorin discs of $A^T$.\n",
    "\n",
    "This  theorem shows that if the off-diagonal entries of a matrix $A$ \n",
    "are small, the eigenvalues cannot be too far from the diagonal entries of \n",
    "$A$. Therefore, if we have an algorithm that reduces the magnitude of \n",
    "the off-diagonal entries, it can be used to approximate the eigenvalues \n",
    "of $A$. Of course, diagonal entries will change in the process of minimizing \n",
    "the off-diagonal entries. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what happens at the off-diagonal entries get larger and larger, \n",
    "consider the matrix:\n",
    "\n",
    "$$C(t) = (1-t)D + tA$$\n",
    "\n",
    "where $D$ is the diagonal of $A$, and $t$ is a parameter between $0$ and $1$.\n",
    "When $t = 0, C(0) = D$ and all the Gershgorin discs are centered at the \n",
    "diagonal entries $a_{ii}$ and have radius zero. When $t=1, C(1)$ is simply \n",
    "$A$. \n",
    "\n",
    "As $t$ increases from $0$ to $1$, it turns out that the radius of the Gershgorin\n",
    "discs increase linearly with $t$, from $0$ to \n",
    "\n",
    "$$\\sum_{j \\neq i} |a_{ij}|$$\n",
    "\n",
    "<img src=\"https://github.com/oceanumeric/Applied-Mathematics/raw/main/images/gershgorin.gif\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the formula,\n",
    "\n",
    "$$C(t) = (1-t)D + tA$$\n",
    "\n",
    "when $t$ is small, all of the discs are still disjoint, each disc contains \n",
    "exactly one eigenvalue, which is close to the corresponding eigenvalue of \n",
    "$A$. As $t$ grows, it could happen that two discs merge. In this case,\n",
    "something special happens. After the merge, the union of both two disc\n",
    "contains exactly two eigenvalues (possibly with algebraic multiplicity 2).\n",
    "These two eigenvalues may lie in both discs or only one of them: one eigenvalue\n",
    "may move from one disc to another after the discs merge. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitarily diagonalizable matrices \n",
    "\n",
    "The eigendecomposition is very useful, but - as we will see later - it can \n",
    "become difficult to compute with when $X$ is ill-conditioned (that is,\n",
    "if the eigenvectors are nearly linearly dependent). one reason is that \n",
    "computing $X^{-1}$ becomes difficult and can lead to large numerical errors. \n",
    "\n",
    "Thus, we might ask ourselves when $X$ is guaranteed to be _well-conditioned_.\n",
    "In that case, $X$ would be orthogonal or unitary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. [In Praise of the Gershgorin Disc Theorem](https://golem.ph.utexas.edu/category/2016/08/in_praise_of_the_gershgorin_di.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (default, Nov 23 2021, 15:27:38) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe189d182e892b13e31889708ff6fa1238858c117c3508e6a10c00d2dc805a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
